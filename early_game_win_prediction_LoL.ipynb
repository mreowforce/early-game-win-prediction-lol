{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8194573-05ee-46a8-8366-c2d837741950",
   "metadata": {},
   "source": [
    "# Can You Predict a Win from the First 10 Minutes?\n",
    "## Predicting League of Legends match outcomes using early-game data  \n",
    "### Simone Dolcecanto \n",
    "\n",
    "## Overview\n",
    "This notebook explores whether the outcome of a *League of Legends* match can be predicted using only information from the first 10 minutes of gameplay.\n",
    "\n",
    "The dataset contains early-game metrics from ~10,000 high-ranked matches. For each match, we have:\n",
    "- **38 features** (19 per team) describing the game state at minute 10  \n",
    "- a binary target label: **`blueWins`** (1 if the blue team wins, 0 otherwise)\n",
    "\n",
    "The workflow is:\n",
    "1. Data preprocessing  \n",
    "2. Exploratory analysis (correlations and distributions)  \n",
    "3. Dimensionality reduction (PCA)  \n",
    "4. Clustering in PCA space  \n",
    "5. Supervised classification (Random Forest) and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sea #pairplot mainy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler #scale the data for PCA\n",
    "from sklearn.model_selection import train_test_split #split into train and test\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01983acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataframe\n",
    "folder_path= ' '\n",
    "file_name= 'high_diamond_ranked_10min.csv'\n",
    "file_path =  folder_path + '/' + file_name \n",
    "lol_total = pd.read_csv(file_path)\n",
    "\n",
    "print(lol_total.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9011e",
   "metadata": {},
   "source": [
    "## Pairplot (sanity check for linear separability)\n",
    "\n",
    "A Seaborn pairplot is a quick way to inspect whether the classes look linearly separable in low-dimensional projections.\n",
    "\n",
    "Here, winning and losing matches are strongly overlapping across most feature combinations, suggesting that **a simple linear separator is unlikely to perform well** without additional feature engineering or non-linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f60db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea.pairplot(lol_total.sample(1000), hue=\"blueWins\") #set hue (5 min graph). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd4ed9f",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET FIRST COL AS ROWNAMES (INDEX)\n",
    "\n",
    "lol_total.set_index('gameId', inplace=True)#inplace means acts on original object\n",
    "print(lol_total) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol_total.isnull().sum()\n",
    "lol_total.duplicated().sum()# no null nor duplicates\n",
    "print(lol_total.isnull().sum(), lol_total.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bbc79f",
   "metadata": {},
   "source": [
    "## Data quality checks\n",
    "\n",
    "The dataset contains **no missing values** and **no duplicate rows**.\n",
    "\n",
    "Next, the data is split into **training** and **test** sets to support fair evaluation of downstream models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lol_train, lol_test = train_test_split(lol_total, test_size=0.3, random_state=42) #i prep the sample (roughly 70%), for model training\n",
    "# 42 is the random key so it always results in the same split\n",
    "print(lol_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001207f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z-SCORE\n",
    "vars = lol_train.drop('blueWins', axis=1)#drop target column. axis is 0 for index and 1 for columns\n",
    "target_var= lol_train[\"blueWins\"]\n",
    "lol_zp = StandardScaler().fit_transform(vars) #take scaler instance, then exec with fit_transform on the vars. p for partial\n",
    "lol_z= pd.DataFrame(lol_zp, columns= vars.columns, index= vars.index ) #make df from the scaled vars otherwise remains numpy array \n",
    "lol_z[\"blueWins\"] = target_var #add target back to the scaled df\n",
    "print(lol_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1faa8b9",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "## Correlation with `blueWins`\n",
    "\n",
    "A correlation matrix helps highlight which early-game variables are most associated with the final outcome (`blueWins`) and also reveals groups of features that are strongly intercorrelated (multicollinearity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acfd891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix with blueWins as the target variable\n",
    "lol_z_corr = lol_z.corr()\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "sea.heatmap(lol_z_corr, annot=True, cmap='Purples', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1944b7d1",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "As expected, the features most associated with `blueWins` capture **relative advantage** between the two teams in the first 10 minutes.\n",
    "\n",
    "In particular, differences in **gold** and **experience** show moderate positive correlation with blue victory. Additionally, **`redDeaths`** correlates positively with `blueWins`, indicating that early combat outcomes often translate into a meaningful advantage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca= PCA(n_components= 0.95)# % of explained variance\n",
    "lol_notarg =lol_z.drop(columns=\"blueWins\")\n",
    "lol_z_pca= pca.fit_transform(lol_notarg)# gets princ components and adjourns df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9991ec7",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is used to reduce dimensionality and make the structure of the data easier to visualize and interpret.\n",
    "\n",
    "A scree plot (explained variance by component) is used with the **elbow method** to select a reasonable number of principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d622a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp_var = pca.explained_variance_ratio_ \n",
    "plt.plot(range(1, len(exp_var)+1), exp_var) # scree plot \n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Proportion of Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()# not enough exp var for this to be useful maybe. see numerical values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c7ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_exp_var = exp_var.cumsum()\n",
    "for i, var in enumerate(exp_var):\n",
    "    print(f\"Variance explained by Principal Component  {i+1}: {var}\")\n",
    "    print(f\"Cumulative exp variance after Principal Component {i+1}: {cum_exp_var[i]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = pca.explained_variance_\n",
    "\n",
    "for i, eigenvalue in enumerate(eigenvalues):\n",
    "    print(f\"Eigenvalue of Principal component {i+1}: {eigenvalue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5517285",
   "metadata": {},
   "source": [
    "## PCA results\n",
    "\n",
    "After scaling the features and running PCA, **4 principal components** are retained using the elbow method.\n",
    "\n",
    "These components explain **~58%** of the total variance. While additional components would capture more variance, they would also make interpretation and visualization less clear.\n",
    "\n",
    "Next, the retained components are explored using 3D biplots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca= PCA(n_components= 4)# adjust numbeer of components\n",
    "lol_pca=pca.fit_transform(lol_notarg)\n",
    "colummns =[\"pca_comp_%i\" %i for i in range(4)]# string formatting: create list of col names\n",
    "lol_pca= pd.DataFrame(lol_pca, columns= colummns, index= lol_notarg.index )\n",
    "\n",
    "lol_pca[\"blueWins\"] = target_var #add back ttarg val\n",
    "\n",
    "print(lol_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorss= {0: 'red', 1: 'blue'}\n",
    "def biplot_2d(pca, data, labels, component_num, target_values): #FINALLY WORKING\n",
    "    components = pca.components_\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    ax.set_xlabel(f'PC{component_num}')\n",
    "    ax.set_ylabel(f'PC{component_num + 1}')\n",
    "\n",
    "    \n",
    "    \n",
    "    for i, label in enumerate(labels[:min(len(labels), len(components[component_num]))]):# oob when choosing min. try max?(no)\n",
    "        \n",
    "        ax.text(components[component_num, i], components[component_num + 1, i], label, color='g')\n",
    "        \n",
    "    \n",
    "    for i, (comp1, comp2) in enumerate(zip(components[component_num], components[component_num + 1])):\n",
    "        ax.plot([0, comp1], [0, comp2], color='r', linewidth=1, linestyle='--')\n",
    "\n",
    "    #ax.scatter(data.iloc[:, component_num], data.iloc[:, component_num + 1], c=target_values, cmap='viridis', s=1.5, alpha=0.5)\n",
    "    #ax.set_xlim(-0.5,0.5)\n",
    "    #ax.set_ylim(-0.5,0.5)\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Call biplot_2d function for each pair of pcs\n",
    "for component_num in range(3):\n",
    "    biplot_2d(pca, lol_pca, lol_notarg.columns, component_num, lol_pca[\"blueWins\"].values.tolist() )#problem was here. +1 after comonent_num for some reason.\n",
    "    #didn't work before because blueWins is a series. it needed a list or an array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bdfa73",
   "metadata": {},
   "source": [
    "## Biplots (interpreting the components)\n",
    "\n",
    "Biplots visualize samples in PCA space while also showing which original variables contribute most to each principal component.\n",
    "\n",
    "A reasonable interpretation is:\n",
    "- **PC0** (highest variance) primarily reflects **overall resource acquisition and advantage**.\n",
    "- **PC1** appears linked to **early combat performance** (kills/deaths-related dynamics).\n",
    "- **PC2** emphasizes **short-term objective control**, such as the acquisition of buffs (e.g., the first herald/dragon dynamics depending on available features).\n",
    "\n",
    "These interpretations are used as qualitative guidance rather than definitive causal claims.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(pca.components_,columns=lol_notarg.columns,index = ['PC0','PC1','PC2','PC3']))\n",
    "#relative contr per column of each pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae90c3",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Clustering provides an additional way to summarize match states by grouping games with similar early-game characteristics.\n",
    "\n",
    "K-Means is applied in PCA space. The number of clusters is selected using the elbow method by examining the **within-cluster sum of squares (WCSS / inertia)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means optimization function\n",
    "def optim_kmeans(data, max_k):#an entire function for the graph, again\n",
    "    means=[]\n",
    "    inertia=[]#voids to append\n",
    "\n",
    "    for k in range(1,max_k): #for each k, to a max of k clusters, apply kmeans with k clusters\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(data) #apply kmeans\n",
    "\n",
    "        means.append(k)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "\n",
    "    \n",
    "    plt.subplots(figsize=(10, 5))\n",
    "    plt.plot(means, inertia, 'o-')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia/WCSS')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "optim_kmeans(lol_pca, 10)#pca dataset of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 clusters is acceptable\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(lol_pca)\n",
    "lol_pcak= lol_pca.copy()\n",
    "lol_pcak[\"kmeans\"]=kmeans.labels_#df of pca with kmeans appended\n",
    "\n",
    "plt.scatter(lol_pcak.iloc[:,0],lol_pcak.iloc[:,1],c=kmeans.labels_,cmap=\"viridis\",alpha=0.2)\n",
    "labels=np.unique(kmeans.labels_)\n",
    "colors=plt.cm.viridis(np.linspace(0,1,len(labels)))\n",
    "plt.xlabel('PC0')\n",
    "plt.ylabel('PC1')\n",
    "plt.legend(handles=[plt.scatter([], [], color=c, label=f'Cluster {i}') for i, c in zip(labels, colors)],\n",
    "           title=\"Clusters\",\n",
    "           loc=\"best\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6bd86",
   "metadata": {},
   "source": [
    "The clusters distribute coherently in PCA space.\n",
    "\n",
    "A high-level reading is:\n",
    "- **Clusters 0–1**: match states dominated by **combat dynamics**\n",
    "- **Clusters 2–3**: match states dominated by **resource accumulation**\n",
    "\n",
    "Next, clustering quality is evaluated using the **silhouette coefficient** and **SSE**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70888b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#internal cluster vlaidity measure: silhouette coeff\n",
    "# avg[(intraclust_dist - interclust_dist)/max(intraclust_dist, interclust_dist)]\n",
    "silhouette_avg= silhouette_score(lol_pcak, kmeans.labels_)\n",
    "print(f\"Silhouette average score on pca'ed dataframe: {silhouette_avg}\")\n",
    "#Silhouette average score on pca'ed dataframe: 0.25717978374136885. abysmal\n",
    "silhouette_avg_orig= silhouette_score(lol_notarg, kmeans.labels_)\n",
    "print(f\"Silhouette average score on scaled train dataframe: {silhouette_avg_orig}\")\n",
    "#Silhouette average score on scaled train dataframe: 0.10824271489828395. unsurprisingly, even worse\n",
    "#SSE, Sum of Squared Errors\n",
    "sse_pca= kmeans.inertia_\n",
    "print(f\"SSE on pca'ed dataframe: {sse_pca}\")\n",
    "#SSE on pca'ed dataframe: 73025.85170924194. a terrible result, but one expected, due to the heavy loss of information following pca.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf5323",
   "metadata": {},
   "source": [
    "The difference in performance between the PCA-transformed space and the fully scaled feature space is noticeable, with an approximate drop of **~15%** in this comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af862789",
   "metadata": {},
   "source": [
    "# Classification: Random Forest\n",
    "\n",
    "A Random Forest classifier is trained to predict `blueWins` from early-game features.\n",
    "\n",
    "Random Forests are a strong baseline for tabular data and handle non-linear relationships well. Hyperparameters are tuned using **GridSearchCV**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8496d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASSIFICATION\n",
    "#grid search\n",
    "grid_search =    GridSearchCV( #use grid search to find hyperparameters for random forest\n",
    "        estimator=RandomForestClassifier(),\n",
    "        param_grid={\n",
    "            'n_estimators': [10, 100, 200],\n",
    "            'max_depth': [None, 5, 10],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        cv=5\n",
    "    )\n",
    "grid_search.fit(lol_train.drop('blueWins', axis=1), lol_train['blueWins'])#ind and dep vars\n",
    "best_params=grid_search.best_params_\n",
    "best_score=grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "#Best Hyperparameters: {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 10}\n",
    "#Best Score: 0.7298626174981924\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ccb8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf= RandomForestClassifier(max_depth=5, min_samples_leaf=4, min_samples_split=10, n_estimators=10, random_state=42)\n",
    "train_feat= lol_train.drop(['blueWins'], axis=1)\n",
    "train_label= lol_train.blueWins\n",
    "rf.fit(train_feat, train_label)#train\n",
    "test_pred= rf.predict(lol_test.drop(['blueWins'], axis=1))\n",
    "\n",
    "print(accuracy_score(lol_test['blueWins'], test_pred))\n",
    "#0.7206477732793523 \n",
    "X_val = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)\n",
    "X_val_res= cross_val_score(rf, train_feat, train_label, cv=X_val, scoring='accuracy')\n",
    "print(f\"average accuracy: {X_val_res.mean()}\")\n",
    "#average accuracy: 0.7262890968103529"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f036f97",
   "metadata": {},
   "source": [
    "The final model does not reach the best cross-validation score observed during grid search, but it achieves an average test accuracy of **~72%**.\n",
    "\n",
    "Repeated Stratified K-Fold validation yields consistent performance (within ~0.006), supporting the stability of this estimate.\n",
    "\n",
    "Given that predictions are made using only the first 10 minutes of matches that can last up to ~1 hour, this is a meaningful result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc833ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880312d4",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix summarizes true/false positives and negatives, helping identify the types of errors the model makes and whether misclassifications are balanced across classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894929bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_mtx = confusion_matrix(lol_test['blueWins'], test_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=con_mtx, display_labels=rf.classes_)\n",
    "disp.plot(cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve\n",
    "prob_t=rf.predict_proba(lol_test.drop(['blueWins'], axis=1))\n",
    "fpr, tpr, thresholds = roc_curve(lol_test['blueWins'], prob_t[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd35879",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "\n",
    "The ROC curve evaluates the classifier across decision thresholds by comparing true positive rate and false positive rate.\n",
    "\n",
    "An **AUC of ~0.81** indicates good discriminative ability when predicting match outcomes using only early-game information.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
